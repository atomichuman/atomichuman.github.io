---
layout: initiative
title: "X-Risk Decomposed: Power and Automation"
date: 2025-01-01
toggle_machine_commentary: true
description: The UK AI policy conversation has been derailed by over-focus on existential risk narratives. But how can we use these narratives more productively and ensure that we don't miss any underlying truths they may contain?
contributed_by:
  initial:
    date: 2024-12-27
    type: human
    person_id: Neil D. Lawrence
    notes: First stub of post.
  reviewed_by:
  - date: 2025-01-01
    type: human
    person_id: Neil D. Lawrence
    notes: First full draft of post.
  - date: 2025-01-01
    type: machine
    tool: Claude (via Cursor)
    version: 3.5-sonnet
    notes: Request to operate like a subeditor marking up potential changes in the post.
  - date: 2025-01-01
    type: human
    person_id: Neil D. Lawrence
    notes: Review first round of machine driven subediting.
featured_image: /assets/images/1280px-Newton-WilliamBlake.jpg
# Featured image path is incomplete 
---

There's a common misconception among [the (technical) x-Risk community](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence) that there has been no thinking or legislation connected with their concerns. This is not true. But it can appear to be true to those who are focussed on particular models of how such risk comes about.

In the book I introduce the notion of [model blinkers](/themes/model-blinkers/). This concept captures our tendency to apply overly reductionist approaches to complex problems. We see this particularly in the x-risk community's thinking around artificial intelligence. 

The solution to model blinkers is to take a step back from the problem and consider broader perspectives. This is where [Wisdom in Diversity](/themes/wisdom-in-diversity/) can help. But we don't always have access to the diverse perspectives we need. Fortunately there are other techniques we can use to help. One approach I use to lift my own thinking is to look for a *broader class of models* that subsumes the specific example that is overly-narrowing the focus. I've probably made that sound more complicated than it is. Let's look at the example of AI x-risk. 

The notion of existential threat from artificial general intelligence suffers from simplistic thinking about the nature of intelligence that originates in eugenics. My approach to stepping back from the simple is inspired by one of my favourite statistical models: principal component analysis.[^1] In principal component analysis the idea is to decompose dataset into its underlying "components". But we can also apply this approach to concepts. Stepping back from AI x-risk we can see that it conflates two important concepts. Both turn out to be important socio-technical challenges that need to be addressed. 

[^1]: Let's be careful though, principal components analysis is inspired by factor analysis, which is the very [technique that Spearman developed and used to support a single dimensional notion of "general intelligence"](https://en.wikipedia.org/wiki/Two-factor_theory_of_intelligence). Despite this, I like principal component analysis not because it is in any sense the best model, but it is one of (or perhaps the) first that should be applied to analysis of most high dimensional data sets. It allows the data scientist to step back from the problem and lift their thinking. 

<center>
<img src="/assets/images/1280px-Newton-WilliamBlake.jpg" width="70%"/>

<i>[William Blake's "Newton"](/images/william-blake-newton/) captures the great scientist with his model blinkers on. Newton's focus on the simple shapes in front of him distract him from the beauty and complexity of the world around him.</i>
</center>

The first challenge is over [*concentration of power*](/themes/power-asymmetries/). In society concentration of power enables us to bring resources together to solve problems. That can make us more efficient for specific tasks, but it comes with the risk of undermining the diversity of opinion in society that keeps us robust to unforeseen challenges. Society already has mechanisms for maintaining the balance between these challenges. We already regulate to deal with power asymmetries that arise from governments, corporations and individuals. Much of this regulation also applies to decision-making machines. 


The second challenge is that of [automated decision-making](/themes/automated-decision-making/). Like power concentration automated decision-making can allow us be more efficient at specific tasks, but it runs the risk of ignoring wider human context which may be less efficient for a given task but considers a broader range of cultural and individual learnings. 

These two components offer a fresh view of the x-risk argument. And we can immediately appreciate that any conflation of these two would present an existential threat regardless of whether or not we believe in the notion of artificial general intelligence. But the broader perspective also shows that we should be worried by individuals, governments, corporations and other institutions concentrating power. Because it's within or by those entities that automated decision-making systems are being developed and deployed. The components also allow for policy or legislation that builds on existing practices but also accounts for the fears of the x-risk community. All this becomes possible when those fears are contextualised within a broader context.[^2]

[^2]: This doesn't mean we should never consider a narrow model like those proposed in the x-risk community. We just need to ensure we move between the broad and narrow approaches, gaining the best of both worlds. 

So what is the existing legislation that can support us in addressing these challenges? For this post I have two examples. Firstly, competition and consumer policy. With this in mind for the last couple of years I've joined the Digital Experts Group of the UK's Competition and Markets Authority (CMA). This group supports the CMA's Digital Markets Unit with technical advice.[^3]

[^3]: As one example of how damaging a narrow obsession with particular risk can be, the [Digital Markets Competition and Consumer Act](https://bills.parliament.uk/bills/3453) which placed the digital markets unit on statutory footing only became law in the last parliament by the narrowest of margins. The UK early election meant that it was passed in the last parliamentary session and achieved Royal Assent on 24th May. For the UK this instrument will prove one of the most important tools in addressing these socio-technical risks. This nerve-wracking moment should have been avoided if attention had been focussed on the policy levers within government's control.[^4] 

[^4]: See also [this letter to the Secretary of State from June 2023.](https://mlatcl.github.io/papers/sos-letter-simplistic-narratives.pdf).

Automated decision-making is also a challenge that has a long history in policy. I recently spoke at the [40th anniversary of the Information Commissioner's Office](https://ico.org.uk/for-the-public/ico-40/). The poorly named "data protection" legislation gives us personal data rights that give us some control of how our personal data can be used to make decisions about us. A better name might be "personal data rights" legislation. The UK's information commissioner, John Edwards, recently reviewed *The Atomic Human* for *Data and Policy*. Quoting from his review.

> > Unfortunately these regulations don’t directly protect us regarding the “inconsequential” decisions that are made about us on a regular basis by social media platforms. Decisions about what posts to highlight in our social media news feed or what adverts to show us. This series of “inconsequential” decisions can accumulate to have a highly consequential effect on our lives and even our democracies. This is a new threat that isn’t directly addressed by existing data rights” (p. 364)
>
> I’m not sure I agree with that final conclusion. It is true that data protection legislation has not been deployed in that way, but it does not necessarily follow that it cannot be.

I agree with John's assessment. It should be possible to deploy data protection law in this manner. And tomorrow's post will look at how data trusts aim to achieve that. 

The x-risk narrative is a form of futurology, one that can draw particular attention because it is associated with powerful narratives about our intelligence that we can see threaded throughout human history. Like any form of futurology it can be useful to think through the implications of the narrative, but modern [futures studies](https://en.wikipedia.org/wiki/Futures_studies) emphasises the importance of considering a plurality of possible futures. It is easy to put on the model-blinkers and proselytise the merits of what those blinkers show us. It is harder to listen to other perspectives and understand the limits of our own thinking. We don't always have access to the diverse voices we need to challenge narrow thinking, but breaking down our model into its components can help us contextualise the model, seeing its limitations alongside its predictions.

<div class="machine-commentary" markdown=1>

## Machine Commentary

*NL*: I'd like you to provide a "Machine Commentary" on how the initiative fits with the ideas in *The Atomic Human* and how its approach informed the ideas in the book.


*NL*: Can you provide further machine commentary on how DSA is fitting with the approach [described in this post](/reflections/purpose-people-projects-principles-process/)?

</div>