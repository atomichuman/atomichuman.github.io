---
layout: reflection
title: "X-Risk Decomposed: Power and Automation"
date: 2025-01-01
toggle_machine_commentary: true
description: Can we decompose these x-risk narratives into more productive routes for AI policy?
contributed_by:
  initial:
    date: 2024-12-27
    type: human
    person_id: Neil D. Lawrence
    notes: First stub of post.
  reviewed_by:
  - date: 2025-01-01
    type: human
    person_id: Neil D. Lawrence
    notes: First full draft of post.
  - date: 2025-01-01
    type: machine
    tool: Claude (via Cursor)
    version: 3.5-sonnet
    notes: Request to operate like a subeditor marking up potential changes in the post.
  - date: 2025-01-01
    type: human
    person_id: Neil D. Lawrence
    notes: Review first round of machine driven subediting.
  - date: 2025-01-01
    type: machine
    tool: Claude (via Cursor)
    version: 3.5-sonnet
    notes: Request to operate like a subeditor marking up potential changes in the post.
  - date: 2025-01-01
    type: human
    person_id: Neil D. Lawrence
    notes: Review second round of machine driven subediting.
  - date: 2025-01-01
    type: machine
    tool: Claude (via Cursor)
    version: 3.5-sonnet
    notes: Third round of subediting suggestions focusing on structure and clarity
  - date: 2025-01-01
    type: human
    person_id: Neil D. Lawrence
    notes: Review third round of machine driven subediting.
featured_image: /assets/images/1280px-Newton-WilliamBlake.jpg
---

"Have you done principal components analysis yet?" is the gentle question I ask to every student who comes to my office keen to show me a set of results found by deploying the latest AI model. These models are powerful and exciting, but they are often misleading. Principal components analysis is a fundamental technique that decomposes a data set into different components. Visualising those components lets the students stand back from their complex model and reflect on whether they are asking the right questions and whether the data can answer those questions.

In *The Atomic Human* I introduce the notion of [model blinkers](/themes/model-blinkers/) to describe the challenge these students are experiencing. Our models of the world can be so entrancing we sometimes forget to step back and ask whether they are valid or not. 
There's a similar challenge with [the (technical) x-Risk community](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_general_intelligence). They are so focussed on their model of how future armagaeddon will emerge that they sometimes fail to acknowledge the wider societal context of their worries. 

The solution to model blinkers is to take a step back from the problem and consider broader perspectives. This is where [wisdom in diversity](/themes/wisdom-in-diversity/) can help - my students are sanity checking their conclusions by coming to me for help. But we don't always have access to the diverse perspectives we need. Fortunately, we can borrow from the approach of principal components analysis and perform a conceptual decomposition of the x-risk narrative into its components.[^1]

[^1]: Let's be careful though, principal components analysis is inspired by factor analysis, which is the very [technique that Spearman developed and used to support a single dimensional notion of "general intelligence"](https://en.wikipedia.org/wiki/Two-factor_theory_of_intelligence). This simple description of intelligence is a good example of a model blinker. Despite this, I like principal component analysis not because it is in any sense the best model, but it is one of (or perhaps the) first that should be applied to analysis of most high dimensional data sets. It allows the data scientist to step back from the problem and lift their thinking. 

<center>
<img src="/assets/images/1280px-Newton-WilliamBlake.jpg" width="70%"/>

<i>[William Blake's "Newton"](/images/william-blake-newton/) captures the great scientist with his model blinkers on. Newton's focus on the simple shapes in front of him distract him from the beauty and complexity of the world around him.</i>
</center>

The x-risk narrative tells of machines which obtain great power and decide upon our fate. We can understand the problem it describes and decompose the concepts into two separate components.

The first component is [*concentration of power*](/themes/power-asymmetries/). This machine has somehow obtained great power. In society concentration of power enables us to bring resources together to solve problems. That can make us more efficient for specific tasks, but it comes with the risk of undermining the diversity of opinion in society that keeps us robust to unforeseen challenges. Society already has mechanisms for maintaining the balance between these challenges. We already regulate to deal with power asymmetries that arise from governments, corporations and individuals. Much of this regulation also applies to decision-making machines. 

The second component is [automated decision-making](/themes/automated-decision-making/). The machine is deploying its power through algorithmic decison-making of some form. As societies have grown in size we have introduced processes (laws, regulations, etc.) to support us in our decisions. Increasingly we are make use of machines to automatically implement these processes. This can make some tasks more efficient, but it also runs the risk of ignoring wider human context which incorporates a broader range of cultural and individual learnings. 

These two components offer a fresh view of the x-risk argument. While we can immediately appreciate that any conflation of these two would present an existential threat regardless of whether or not we believe in the notion of artificial general intelligence. 

But the broader perspective also shows that we should be worried by individuals, governments, corporations and other institutions concentrating power.[^2] How do these components the inform AI policy? Let's look at two examples for UK AI policy: competition and consumer policy and "data protection".

[^2]: This doesn't mean we should never consider a narrow model like those proposed in the x-risk community. We just need to ensure we move between the broad and narrow approaches, gaining the best of both worlds. 

Firstly, let's look at concentration of power and how that's dealt with in consumer markets. Competition and consumer policy is a natural fit for the first component: it deals a regulatory framework to manage concentration of power in markets. With this in mind, two years ago I joined the Digital Experts Group of the UK's Competition and Markets Authority (CMA) to support the CMA's Digital Markets Unit with technical advice.[^3] The Digital Markets Unit focusses on the new ways in which power concentrates in digital markets. Decomposing the x-risk narrative demonstrates the importance of addressing concentration of power in markets.

[^3]: As one example of how damaging a narrow obsession with particular risk can be, the [Digital Markets Competition and Consumer Act](https://bills.parliament.uk/bills/3453) which placed the digital markets unit on statutory footing only became law in the last parliament by the narrowest of margins. The UK early election meant that it was passed in the last parliamentary session and achieved Royal Assent on 24th May. For the UK this instrument will prove one of the most important tools in addressing these socio-technical risks. This nerve-wracking moment should have been avoided if attention had been focussed on the policy levers within government's control.[^4] 

[^4]: See also [this letter to the Secretary of State from June 2023.](https://mlatcl.github.io/papers/sos-letter-simplistic-narratives.pdf).

Secondly data protection legislation. It addresses both components: power asymmetries and automated decision-making. In the UK the Information Commisioner's Office is our data protection regulator. They recently celeberated their [40th anniversary](https://ico.org.uk/for-the-public/ico-40/). Data protection is a poor name. In reality the legislation gives us personal data rights that give us control of how our personal data is used to make decisions about us. This should also give us some power to address power asymmetries behind automated decision-making. In *The Atomic Human* I make the point that so far that hasn't panned out quite as we might have hoped. Addressing this is one reason I'm interested in the model of data trusts.

The UK's information commissioner, John Edwards, reviewed [*The Atomic Human* for *Data and Policy*](https://medium.com/data-policy/the-atomic-human-understanding-ourselves-in-the-age-of-ai-by-neil-lawrence-df97c47aaa74). His review builds on the vision laid out in *The Atomic Human* of data protection legislation as a critical regulatory tool for AI policy. He quotes the following passage from the book.

> Unfortunately these regulations don't directly protect us regarding the "inconsequential" decisions that are made about us on a regular basis by social media platforms. Decisions about what posts to highlight in our social media news feed or what adverts to show us. This series of "inconsequential" decisions can accumulate to have a highly consequential effect on our lives and even our democracies. This is a new threat that isn't directly addressed by existing data rights" (p. 364)

And shares his thoughts

> I'm not sure I agree with that final conclusion. It is true that data protection legislation has not been deployed in that way, but it does not necessarily follow that it cannot be.

I agree with John's assessment. I suspect the difference of opinion is around the word "directly". In tomorrow's post I'll introduce the Data Trusts Initiative and explain how I believe we need new institutions to make the vision John and I share a reality. 

The x-risk narrative is a form of futurology, one that draws attention because it feeds our egocentricities about our form of intelligence. Like any form of futurology the x-risk narrative encourages us to ask questions about our future. But the modern field of [Futures Studies](https://en.wikipedia.org/wiki/Futures_studies) emphasises the importance of considering a plurality of possible futures. It is easy to put on the model-blinkers and proselytise the merits of what they show us. But that makes it harder to listen to other perspectives and reflect on the limits of our own thinking. We don't always have access to the diverse voices we need to challenge narrow thinking, but breaking down our model into its components can help us contextualise the model, seeing its limitations alongside its predictions.

<div class="machine-commentary" markdown=1>

## Machine Commentary

*NL*: I'd like you to provide a "Machine Commentary" on how the initiative fits with the ideas in *The Atomic Human* and how its approach informed the ideas in the book.


*NL*: Can you provide further machine commentary on how DSA is fitting with the approach [described in this post](/reflections/purpose-people-projects-principles-process/)?

</div>