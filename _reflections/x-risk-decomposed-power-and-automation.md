---
layout: initiative
title: "X-Risk Decomposed: Power and Automation"
date: 2025-01-01
toggle_machine_commentary: true
description: The Data Trusts Initiative is a project to support pilots in understanding the challenges associated with creating data trusts.
website: https//datatrusts.uk/
contributed_by:
  initial:
    date: 2024-12-27
    type: human
    person_id: Neil D. Lawrence
    notes: First stub of post.
featured_image: /assets/images/
---

There's a common misconception among [the (technical) x-Risk community](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence) that there has been no thinking or legislation connected with their concerns. This is not true. But it can appear to be true to those who are focussed on a particular models of how such risk comes about.

In the book I introduce the notion of [model blinkers](/themes/model-blinkers/) which captures our tendency to apply overly reductionist approaches to complex problems. The solution to model blinkers is to take a step back from the problem and consider broader perspectives. This is where [Wisdom in Diversity](/themes/wisdom-in-diversity/) can help. But there are other approaches. One is to look for a broader class of models that subsumes the specific case that is narrowing the focus.

From a technical, data science perspective, my favourite model is principle component analysis. Not because it's the best model, but because it's one of (or perhaps the first) that should be applied to any high dimensional data. It allows the data scientist to step back from the problem and lift their thinking. 

<center>
<image src="/assets/images/" width="70%"/>

<i></i>
</center>

The model is for a data set, but conceptually I like to translate it into the socio-techical contexts that surround the process of institution building. 

The notion of existential threat from artificial general intelligence suffers from simplistic thinking about the nature of intelligence that originates in eugenics. But stepping back from it we can see that it conflates two important socio-technical risks that need to be addressed. I see these as the two principle components of the existential risk argument. The first is [*power asymmetry*](/themes/power-asymmetries/). The idea that there's an entity (here we can consider it to be a person, a government, a corporation or a decision making machine) that benefits from a concentration of power. The second is the challenge of [automated decision-making](/themes/automated-decision-making/). 

These two principle components offer a fresh view of the existential risk argument. A conflation of the two would be very worrying. But the broader perspective also shows that we should be worried by individuals, governments, corporations and machines concentrating power. These components remove the problematic notion of artificial general intelligence from the discussion. But the still allow for policy or legislation that accounts for the fears of the x-risk community. They simply require those fears to be grounded in a broader context. 

The also allow us to place the policy arguments in a broader context that has dealt with these challenges in the past. For example, competition policy. It was striking, and shocking, to see how little acknowledgement the previous government made of potential the importance of the [Digital Markets Competition and Consumer Act](https://bills.parliament.uk/bills/3453) in dealing with power asymmetries that arise through the concentration of data (and more recently compute). For the UK this instrument will prove one of the most important tools in addressing these socio-technical risks.[^1] The early UK election also meant that the legislation almost didn't pass in time. It achieved Royal Assent on 24th May, 

[^1]: I serve on the digital experts group for the Digital Markets Unit of the UK Competition and Markets Authority.

Automated decision-making is also a challenge with a long history. I recently spoke at the [40th anniversary of the Information Commisioner's Office](https://ico.org.uk/for-the-public/ico-40/). The poorly named "data protection" legislation gives us pesonal data rights that give us some control of how our personal data can be used to make decisions about us. A better name might be "personal data rights" legislation. The UK's information commissioner, John Edwards, recently reviewed *The Atomic Human* for *Data and Policy*. Quoting from his review.

> > Unfortunately these regulations don’t directly protect us regarding the “inconsequential” decisions that are made about us on a regular basis by social media platforms. Decisions about what posts to highlight in our social media news feed or what adverts to show us. This series of “inconsequential” decisions can accumulate to have a highly consequential effect on our lives and even our democracies. This is a new threat that isn’t directly addressed by existing data rights” (p. 364)
>
> I’m not sure I agree with that final conclusion. It is true that data protection legislation has not been deployed in that way, but it does not necessarily follow that it cannot be.

I agree with John's assessment. It should be possible to deploy data protection law in this manner. And tomorrow's post will look at how Data Trusts aim to achieve that. 



<div class="machine-commentary" markdown=1>

## Machine Commentary

*NL*: I'd like you to provide a "Machine Commentary" on how the initiative fits with the ideas in *The Atomic Human* and how its approach informed the ideas in the book.


*NL*: Can you provide further machine commentary on how DSA is fitting with the approach [described in this post](/reflections/purpose-people-projects-principles-process/)?

</div>
