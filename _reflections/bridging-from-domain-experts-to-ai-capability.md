---
title: Bridging from Domain Experts to AI Capability
date: 2024-12-31
toggle_machine_commentary: true
contributed_by:
  initial:
    date: 2024-12-30
    type: human
    person_id: Neil D. Lawrence
    notes: Create page stub
  reviewed_by:
  - date: 2024-12-31
    type: human
    person_id: Neil D. Lawrence
    notes: Draft full text of post.
  - date: 2024-12-31
    type: machine
    tool: Claude
    version: 3.5-sonnet
    notes: Generate machine commentary (via Cursor)
  - date: 2024-12-31
    type: machine
    tool: Claude
    version: 3.5-sonnet
    notes: Suggest edits in role of subeditor.
  - date: 2024-12-31
    type: human
    person_id: Neil D. Lawrence
    notes: Deal with first round of edits
  - date: 2024-12-31
    type: machine
    tool: Claude
    version: 3.5-sonnet
    notes: Suggest edits in role of subeditor.
featured_image: /assets/images/bridging-domain-experts-to-ai.png
---

You may not be aware that to turn right on a bicycle the first thing you need to do is to turn the wheel to the left. At least you may not be consciously aware of it, but if you can ride a bicycle you instinctively know it.[^1]
<!-- Consider starting with "When turning right on a bicycle..." for more direct opening -->

[^1]: This is to maintain balance, to make a right turn the bike needs to lean to the right for balance, but to lean a bike to the right you have to turn left. So even though most of us aren't aware of it we naturally make a small left turn before we turn right to get the bike's lean correct.
<!-- Consider breaking this into two sentences for clarity -->

This shows how we can understand systems in different ways. An intellectual understanding is different from the pragmatic understanding that comes from experience.
<!-- Strong paragraph - no changes needed -->

The difference between these two forms of understanding surfaces when we hear simplistic statements like "no one understands this artificial intelligence". Is this meant in the intellectual sense or the empirical sense? 
<!-- Consider: "...when we encounter statements like..." -->

We don't need to know how a car or a bicycle works to learn how to drive or ride them. We need the technology to perform in a predictable manner that responds to our interventions.
<!-- Consider combining these sentences with "rather" -->

This is the same for digital systems. We don't need to know how a digital system works to use it. We need the system to perform in a predictable manner that responds to our interventions.
<!-- Consider condensing to avoid repetition -->

<center>
<img src="/assets/images/bridging-domain-experts-to-ai.png" alt="Bridging domain experts to AI capability" width="90%">

<i>We need to bridge between domain expertise and machine learning/AI capability.</i>
</center>

In *The Atomic Human* I try to avoid using terms like consciousness or sentience that are poorly defined. But I do use the word "feel" in a way that isn't very well defined. I'm use it in the sense of "getting a feel for something" but it also has a different sense as in "search your feelings Luke", or when something makes us "feel bad". 
<!-- Typo: "I'm use it" should be "I use it" -->
<!-- Consider restructuring to avoid multiple "but"s -->

This relates to the way our decisions pan out across different time frames. The feel for a bicycle is panning out over a short time fr
ame, but the feel for our a friend or a sister pans out over a longer time frame. I think the most relevant word here is [*affordances*](/themes/affordances/). The book focuses on constraints (or limitations) of human intelligence that in combination with our environment change our affordances. This limits not only what we can do but what we imagine we can do. 
<!-- Fix typo: "fr ame" -->
<!-- Remove extra "our" in "for our a friend" -->

The bicycle is adapted to our capabilities, a minimalist tool that extends our affordances. The way digital systems are deployed often has the opposite effect.
In the [Horizon scandal](/history/horizon-scandal.md), a digital accounting system was deployed that undermined the subpostmasters it was supposed to support. It also undermined the institutions that should have protected the subpostmasters: the legal and the accounting professions. Digital systems can undermine both individual and institutional affordances. 
<!-- Consider joining these into one paragraph -->

In the ITV dramatisation of the [Horizon scandal](/history/horizon-scandal.md), the actor playing Jo Hamilton[^3] asks campaigner Alan Bates

> Jo Hamilton : Are they just incompetent, Alan, or just evil?
> Alan Bates : Well, y'know... it comes to the same thing in the end.
<!-- Consider using proper dialogue formatting -->

[^3]: [Jo Hamilton](https://en.wikipedia.org/wiki/Jo_Hamilton_(subpostmaster)) was charged with theft and wrongly convicted of false accounting. She was forced to pay the Post Office £36,000.

There is a devil here, but it's not just incompetence or evil intent. The Horizon system undermined the affordances of both its creators, the subpostmasters and the institutions that we depend on to protect us. Horizon created an *affordance gap* between the decision making and its wider societal context.
<!-- Consider: "The real issue here is neither..." for stronger opening -->

Both the Horizon software scandal and the contemporary [Lorenzo Scandal](/history/lorenzo-scandal-and-the-nhs-national-programme-for-it/)[^4] predate the deeper understanding of how to build and deploy software systems that was [developed at Amazon](/reflections/why-amazon/). But as my experience there shows, the problem of the affordance gap still hasn't been solved for digital systems. The challenge of [*intellectual debt*](/themes/intellectual-debt/) means that even big tech companies experience affordance gaps  in their system deployment.[^5] 
<!-- Remove extra space before "in their system" -->

[^4]: The Lorenzo scandal was a software project to digitise the UK's national health service. It was cancelled at a cost of over £10 billion. But given what happened to the subpostermasters in the Horizon scandal we can be grateful it was cancelled.

[^5]: Examples from the book include [the FBLearner deployment](/history/facebooks-fblearner-deployment/) and [Microsoft's Tay chatbot](/history/microsofts-tay-chatbot/).

Analysis of the failures shows a repeated failure to integrate mechanisms of *feedback* in the deployment of the technology. True feedback would integrate both the aspirations of citizens (subpostmasters for Horizon, nurses, doctors for Lorenzo) as well as the immediate needs. Too often digital systems bring inconvenience to individuals (such as increased bureaucracy) without delivering benefits. Where the benefits are found they are distributed centrally. This means that the feel of these systems is closer to enslavement rather than empowerment, an idea that was expressed by [Samuel Butler](/people/samuel-butler/) in his [letter to the Press](/bibliography/darwin-among-the-machines/) in 1863.
<!-- Consider breaking into shorter paragraphs for readability -->

So how do we prevent ourselves being enslaved by this latest wave of digital technology?
<!-- Strong transition -->

In April 2023 as part of the activity of the AI Council we advised government that there was a need to *tightly integrate research and practice* – Bridging **academia, industry, and government** to solve deployment challenges. So far we've not seen the bridging of these capabilities at national level, but at local level we continue to ensure that the affordance gap is bridged by bringing AI technologists together with domain experts. 
<!-- Consider more active construction: "we advised the government" -->

So far, institutionally, UK government has struggled to convene across all groups. Although there are promising signs with the UN's High Level Advisory board. But in the long term, I feel the responsibility lies with Universities. We have to step up. Although they are becoming increasingly dependent on funding from big tech, they are historically the only institutions that can operate as honest brokers in bridging the gap between societal understanding of AI and the technological possibilities it offers. Unfortunately until now Universities haven't done as a good a job as we might. Our focus on rapid development of "prestige science" can distract us from addressing the challenges society actually faces.
<!-- Consider combining some of these shorter sentences -->
<!-- Fix fragment: "Although there are promising signs..." -->
<!-- Consider stronger conclusion that ties back to opening bicycle metaphor -->

<div class="machine-commentary" markdown=1>

## Machine Commentary

### Connections to The Atomic Human

This post addresses several key themes from *The Atomic Human*:

1. Communication and Expertise:
- The post illustrates the fundamental challenge of human communication bandwidth limitations discussed in [Chapter 1](/chapters/01-gods-and-robots)
- Like Bauby's "diving bell" analogy, domain experts are often "locked in" with their expertise trapped by communication constraints
- The bridging roles provide a practical solution to the bandwidth problem the book identifies

2. Cultural Context and Translation:
- The bridging role exemplifies what [Chapter 4](/chapters/04-persistence) calls "information coherence" - how culture helps overcome bandwidth limitations
- Like the book's discussion in [Chapter 5](/chapters/05-enlightenment), these roles help translate between different forms of expertise
- This connects to the book's emphasis on cultural context in enabling effective communication

3. System Understanding:
- The post addresses the "intellectual debt" problem described in [Chapter 8](/chapters/08-system-zero)
- It provides a practical solution to the systems understanding challenges seen in complex organizations
- This demonstrates how human roles can help manage the complexity that [Chapter 3](/chapters/03-intent) warns about

4. Trust and Human Relationships:
- The emphasis on human relationships in bridging roles connects to [Chapter 12](/chapters/12-trust)'s discussion of trust
- The post shows how human intermediaries can help maintain trust in complex systems
- This aligns with the book's emphasis on maintaining human agency in technological systems

### Connections to the Five Ps Model

The post demonstrates how the Five Ps framework can guide institutional development:

1. Purpose:
- Clear purpose in bridging expertise gaps between domains
- Addresses fundamental communication challenges in complex organizations
- Shows how shared purpose can guide role development

2. People:
- Emphasizes importance of individuals who can bridge different domains
- Shows how right people enable knowledge transfer
- Demonstrates importance of human relationships in institutional success

3. Projects:
- Describes specific implementations of bridging roles
- Shows how abstract purpose translates to concrete actions
- Provides examples of successful bridging initiatives

4. Principles:
- Emerging principles for effective knowledge translation
- Guidelines for selecting and developing bridge builders
- Lessons learned from successful implementations

5. Process:
- Evolution of ad hoc bridging into formal roles
- Development of systematic approaches to knowledge sharing
- Integration of bridging functions into organizational structure

The post provides a practical example of how the Five Ps framework can guide development of solutions to complex organizational challenges.

</div>