---
title: Bridging from Domain Experts to AI Capability
date: 2024-12-31
toggle_machine_commentary: true
contributed_by:
  initial:
    date: 2024-12-30
    type: human
    person_id: Neil D. Lawrence
    notes: Create page stub
featured_image: /assets/images/bridging-domain-experts-to-ai.png
---

One consistent misrepresentation I hear around AI, particularly from those who are creating it is "no one understands this technology". 

Strictly speaking it might be true, but only in the same way that "no one understands Microsoft Word" or "no one understands how a Ford Focus works". 

These systems are very complex, but even for simple systems this is true. Most people aren't aware that to initiate a right turn on a bicycle you first turn the wheel left.[^1]

[^1]: This is to maintain balance, to make a right turn the bike needs to lean to the right for balance, but to lean a bike to the right you have to turn left. So even though most of us aren't aware of it we naturally make a small left turn before we turn right to get the bike's lean correct.

This leads to a suspicion that what people really mean by "no one understands this technology" is "it's really poorly designed". 

We don't need to know how a car or a bicycle works to learn how to drive or ride them. We need the technology to perform in a predictable manner that responds to our interventions.

In *The Atomic Human* I try to avoid using terms like consciousness or sentience that are poorly defined. But I do use the word "feel" a lot in a way that isn't very well defined. I'm using this word in the sense of "getting a feel for something" but it has a different sense as in "search your feelings Luke", or when something makes us "feel bad". 

I think the most relevant word here is [*affordances*](/themes/affordances/). The book focuses on constraints (or limitations) of human intelligence that in combination with our environment change our affordances. This limits not only what we can do but what we imagine we can do. 

The bicycle is adapted to our capabilities, a minimalist tool that extends our affordances. The way digital systems are deployed often has the opposite effect.

In the [Horizon scandal](/history/horizon-scandal.md), a digital accounting system was deployed that undermined the subpostmasters it was suppoed to support. It also undermined the institutions that should have protected the subpostmasters: the legal and the accounting professions. Digital systems can undermine both individual and institutional affordances. 

In the ITV dramatisation of the [Horizon scandal](/history/horizon-scandal.md), the actor playing Jo Hamilton[^3] asks campaigner Alan Bates

> Jo Hamilton : Are they just incompetent, Alan, or just evil?
> Alan Bates : Well, y'know... it comes to the same thing in the end.

[^3]: [Jo Hamilton](https://en.wikipedia.org/wiki/Jo_Hamilton_(subpostmaster)) was charged with theft and wrongly convicted of false accounting. She was forced to pay the Post Office £36,000.

The third alternative that the system undermined the affordances of both its creators, consumers and the institutions that depend on it isn't discussed but as the dialogue from the dramatisation suggests, it still comes to the same thing in the end.

Both the Horizon software scandal and the contemporary [Lorenzo Scandal](/history/lorenzo-scandal-and-the-nhs-national-programme-for-it/)[^4] predate the deeper understanding of how to build and deploy software systems that was [developed at Amazon](/reflections/why-amazon/). But as my experience there shows, the problem of the affordance gap hasn't been solved for digital systems. And [*intellectual debt*](/themes/intellectual-debt/) means that the companies themselves also experience an affordance gap in their system deployment. 

[^4]: The Lorenzo scandal was a software project to digitise the UK's national health service. It was cancelled at a cost of over £10 billion. But given what happened to the subpostermasters in the Horizon scandal we can be grateful it was cancelled.

Analysis of the failures shows a repeated failure to integrate mechanisms of *feedback* in the deployment of the technology. True feedback would integrate both the aspirations of citizens (subpostmasters for Horizon, nurses, doctors for Lorenzo) as well as the immediate needs. Too often digital systems bring inconvenience to individuals (such as increased bureaocracy) without delivering benefits. Where the benefits are found they are distributed centrally. This means tha the feel of these systems is closer to enslavement rather than empowerment, an idea that was expressed by [Samuel Butler in his letter to the Press](/history/samuel-butler-letter-to-the-press/) in Tk.

So what's the solution?

In April 2023 as part of the activity of the AI Council we advised government that there was a need to *tightly integrate research and practice* – Bridging **academia, industry, and government** to solve deployment challenges. So far we've not seen the bridging of these capabilities at national level, but at local level we continue to ensure that the affordance gap is bridged by bringing AI technologists together with domain experts. 

So far, institutionally, UK government has struggled to convene across all groups. Although there are promising signs with the UN's High Level Advisory board. But in the long term, I feel the responsibility lies with Universities. We have to step up. Although they are becoming increasingly dependent on funding from big tech, they are historically the only institutions that can operate as honest brokers in bridging the gap between societal understanding of AI and the technological possibilities it offers. Unfortunately until now Universities haven't done as agood a job as we might. Our focus on rapid development of "prestige science" can distract us from addressing the challenges society actually faces.

