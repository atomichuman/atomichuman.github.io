---
layout: initiative
title: Data Trusts Initiative - I
date: 2025-01-01
toggle_machine_commentary: true
description: The Data Trusts Initiative is a project to support pilots in understanding the challenges associated with creating data trusts.
website: https//datatrusts.uk/
contributed_by:
  initial:
    date: 2024-12-27
    type: human
    person_id: Neil D. Lawrence
    notes: First stub of post.
featured_image: /assets/images/
---

There's a common misconception among [the (technical) x-Risk community](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence) that there has been no thinking or legislation connected with their concerns. This is not true. But it can appear to be true to those who are focussed on a particular models of how such risk comes about.

In the book I introduce the notion of [model blinkers](/themes/model-blinkers/) which captures our tendency to apply overly reductionist approaches to complex problems. The solution to model blinkers is to take a step back from the problem and consider broader perspectives. This is where [Wisdom in Diversity](/themes/wisdom-in-diversity/) can help. But there are other approaches. One is to look for a broader class of models that subsumes the specific case that is narrowing the focus.

From a technical, data science perspective, my favourite model is principle component analysis. Not because it's the best model, but because it's one of (or perhaps the first) that should be applied to any high dimensional data. It allows the data scientist to step back from the problem and lift their thinking. 

The model is for a data set, but conceptually I like to translate it into the socio-techical contexts that surround the process of institution building. 

The notion of existential threat from artificial general intelligence suffers from simplistic thinking about the nature of intelligence that originates in eugenics. But stepping back from it we can see that it conflates two important socio-technical risks that need to be addressed. I see these as the two principle components of the existential risk argument. The first is *power asymmetry*. The idea that there's an entity (here we can consider it to be a person, a government, a corporation or a decision making machine) that benefits from a concentration of power. The second is the challenge of automated decision-making. This is the 
The principle compoents of the exsiTranslating that into our institutional approach
In [yesterday's post](/reflections/bridging-from-domain-experts-to-ai-capability/) I talked about the challenges of integrating [affordances](/themes/affordances) with digital technologies.

Data Science Africa and Accelerate Science both empower through capability sharing. Both initiatives bridge from technical capability in AI to domain expertise that covers a broad spectrum from Ugandan crop surveys to  agriculture or heal

<center>
<image src="/assets/images/" width="70%"/>

<i></i>
</center>


<div class="machine-commentary" markdown=1>

## Machine Commentary

*NL*: I'd like you to provide a "Machine Commentary" on how the initiative fits with the ideas in *The Atomic Human* and how its approach informed the ideas in the book.


*NL*: Can you provide further machine commentary on how DSA is fitting with the approach [described in this post](/reflections/purpose-people-projects-principles-process/)?

</div>
